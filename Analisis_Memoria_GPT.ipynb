{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkmkVXflk3KEnEZPa5+z9O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebabecerra/CMF_Memorias/blob/main/Analisis_Memoria_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seGqlzSxDVe3"
      },
      "outputs": [],
      "source": [
        "# Entorno y gestion de paquetes\n",
        "!pip install spacy --quiet\n",
        "#!pip spacy download es_core_news_sm --quiet\n",
        "!python -m spacy download es_core_news_md --quiet\n",
        "!python -m spacy download es_core_news_sm --quiet\n",
        "\n",
        "!sudo apt install build-essential libpoppler-cpp-dev pkg-config python3-dev\n",
        "!pip install pdftotext\n",
        "!pip install PyPDF2\n",
        "!pip install pdfminer.six\n",
        "!sudo apt install build-essential libpoppler-cpp-dev pkg-config python3-dev\n",
        "!pip install pdftotext\n",
        "!pip install PyPDF2\n",
        "!pip install pdfminer.six\n",
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración inicial\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Importación de librerías\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "import string\n",
        "import os\n",
        "import re\n",
        "from IPython.display import Image\n",
        "from itertools import compress\n",
        "from PyPDF2 import PdfReader\n",
        "import pdftotext\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import openai\n",
        "import unicodedata\n",
        "from string import digits\n"
      ],
      "metadata": {
        "id": "v255FjTKET_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar recursos de NLTK (solo se ejecuta una vez)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Cargar stopwords en español\n",
        "stopword_es = set(nltk.corpus.stopwords.words('spanish'))\n",
        "\n",
        "# Cargar el modelo de spaCy en español\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definir rutas y crear directorio\n",
        "project_dir = '/content/drive/MyDrive/RedSemantica'\n",
        "pdf_url = \"https://cdn3.bci.cl/uploads/fdadf18f-b581-4548-bec2-e97750800f2e/original/Mem_Bci_2023_c.pdf\"\n",
        "pdf_filename = \"Memoria_BCI_2023.pdf\"\n",
        "txt_filename = 'bci.txt'\n",
        "\n",
        "# Crear el directorio de trabajo si no existe\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "\n",
        "# Cambiar el directorio actual al directorio del proyecto\n",
        "os.chdir(project_dir)\n",
        "\n",
        "# Descargar el archivo PDF solo si no existe\n",
        "if not os.path.exists(pdf_filename):\n",
        "    os.system(f\"wget -O {pdf_filename} {pdf_url}\")\n",
        "\n",
        "# Convertir el PDF a texto si no se ha hecho antes\n",
        "if not os.path.exists(txt_filename):\n",
        "    with open(pdf_filename, \"rb\") as f:\n",
        "        pdf = pdftotext.PDF(f)\n",
        "    with open(txt_filename, 'w') as f:\n",
        "        f.write(\"\\n\\n\".join(pdf))\n",
        "\n",
        "# Leer y procesar el texto desde el archivo generado\n",
        "with open(txt_filename, 'r') as f:\n",
        "    texto = f.read()"
      ],
      "metadata": {
        "id": "H2Ja7_OBEUdQ",
        "outputId": "e7159eca-3e76-4dd3-cad6-700a44248817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ampliar la lista de stopwords con términos específicos\n",
        "custom_stopwords = {\n",
        "    \"artículo\", \"capítulo\", \"decreto\", \"n°\", '°', \"inciso\", \"dicha\", \"año\",\n",
        "    \"fija\", \"constitucion\", \"politica\", \"republica\", \"ministerio\", \"secretaría\",\n",
        "    \"general\", \"presidencia\", \"publicación\", \"promulgación\", \"tipo\", \"versión\",\n",
        "    \"última\", \"ultima\", \"modificación\", \"biblioteca\", \"congreso\", \"nacional\",\n",
        "    \"página\", \"documento\", \"www.leychile.cl\", \"generado\", \"agosto\", \"bcnclitu\",\n",
        "    \"septiembre\", \"refiere\", \"ley\", \"orgánica\", \"plazo\", \"siguientes\", \"miembros\",\n",
        "    \"siguiente\", 'inciso', 'número', 'efecto', 'fecha', 'años', 'procederá',\n",
        "    'mayo', 'dichas', 'formas', \"incisos\", 'deberán', 'deber', 'º', 'nº', 'ámbito',\n",
        "    'ámbitos', 'áreas', 'velar', 'ad', 'única', 'únicas', 'único', 'útiles',\n",
        "    'constitución', 'constitucional', 'deberá', 'determinará', 'chile', 'ser',\n",
        "    'por', 'ningún', 'caso', 'según', 'corresponda', 'treinta', 'días',\n",
        "    'funcionarios', 'funcionarias', 'diputados', 'diputadas', 'ministros',\n",
        "    'ministras', 'consejeros', 'consejeras', 'autónomo', 'autónomos', 'organismo',\n",
        "    'organismos', 'trabajadores', 'trabajadoras', 'jueces', 'juezas', \"medidas\",\n",
        "    \"necesarias\", \"podrá\", \"establecer\", \"demás\", \"atribuciones\", \"disposición\",\n",
        "    \"transitoria\", \"medidas\", \"necesarias\", \"texto\", \"refundido\",\n",
        "    \"disposiciones\", \"transitorias\", \"vigentes\", \"ratificados\", \"política\",\n",
        "    \"república\", 'preámbulo', 'mayoria', 'presidente', 'presidentes', \"presidenta\",\n",
        "    'presidentas', \"adolecentes\", \"niños\", \"niñas\", 'servicio', 'electoral',\n",
        "    \"memoria\", \"bci\", \"reportes\", \"banco\", \"reporte\",\n",
        "}\n",
        "stopword_es.update(custom_stopwords)\n",
        "\n",
        "# Cargar el modelo de spaCy en español\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    # Convertir a minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # Tokenización y eliminación de stopwords\n",
        "    text_tokens = word_tokenize(texto)\n",
        "    tokens_without_sw = [word for word in text_tokens if word not in stopword_es]\n",
        "\n",
        "    # Reunir tokens en texto, separando solo por un espacio\n",
        "    texto = \" \".join(tokens_without_sw)\n",
        "\n",
        "    # Eliminación de puntuación, dígitos y tildes\n",
        "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
        "    texto = texto.translate(str.maketrans('', '', string.digits))\n",
        "    texto = ''.join(c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "    # Eliminar instancias específicas\n",
        "    texto = texto.replace('º', '').replace('ª', '').replace('mm', '')\n",
        "    texto = re.sub(r'\\bnd\\b', '', texto)\n",
        "\n",
        "    # Filtrar nuevamente para eliminar palabras de longitud 1 después de limpiar\n",
        "    text_tokens = word_tokenize(texto)\n",
        "    texto = \" \".join([word for word in text_tokens if len(word) > 1])\n",
        "\n",
        "    # Limpieza adicional de espacios y saltos de línea\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "\n",
        "    return texto\n",
        "\n",
        "\n",
        "# Analizar texto y generar DataFrame de tokens\n",
        "def analizar_texto(texto_filtrado):\n",
        "    tokens_texto = word_tokenize(texto_filtrado)\n",
        "    df_tokens_clean_texto = pd.DataFrame(tokens_texto, columns=['token'])\n",
        "\n",
        "    # Frecuencia de palabras\n",
        "    tokens_count_texto = df_tokens_clean_texto['token'].value_counts().rename_axis('token').reset_index(name='counts')\n",
        "    tokens_count_texto = tokens_count_texto[tokens_count_texto['counts'] > 3]\n",
        "\n",
        "    return df_tokens_clean_texto, tokens_count_texto\n",
        "\n",
        "# Generar nube de palabras\n",
        "def generar_wordcloud(tokens_count, title):\n",
        "    # Use 'ngram' as the key for bigram DataFrames\n",
        "    key = 'ngram' if 'ngram' in tokens_count.columns else 'token'\n",
        "    data = dict(zip(tokens_count[key].tolist(), tokens_count['counts'].tolist()))\n",
        "    wc = WordCloud(background_color='white', width=800, height=400, max_words=500).generate_from_frequencies(data)\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(title, fontsize=20, loc=\"center\", backgroundcolor='white', color='black', fontstyle='italic')\n",
        "    plt.show()\n",
        "\n",
        "# Procesamiento de bigramas (NOUN-ADJ)\n",
        "def bigramas_noun_adj(df_tokens_clean_texto, nlp):\n",
        "    tokens_texto = df_tokens_clean_texto['token'].tolist()\n",
        "    mytext = nlp(\" \".join(tokens_texto))\n",
        "\n",
        "    tokens_texto = [token.text for token in mytext]\n",
        "    tags_texto = [token.tag_ for token in mytext]\n",
        "\n",
        "    bi_tokens_texto = [' '.join(x) for x in list(nltk.bigrams(tokens_texto))]\n",
        "    bi_tag_texto = [' '.join(x) for x in list(nltk.bigrams(tags_texto))]\n",
        "\n",
        "    # Filtrar bigramas NOUN-ADJ\n",
        "    data_tag_texto = pd.DataFrame({'token_bi': bi_tokens_texto, 'tag_bi': bi_tag_texto})\n",
        "    data_tag_texto = data_tag_texto[data_tag_texto[\"tag_bi\"] == \"NOUN ADJ\"]\n",
        "\n",
        "    return data_tag_texto\n",
        "\n",
        "# Ejecución principal\n",
        "def main(texto):\n",
        "    texto_filtrado = limpiar_texto(texto)\n",
        "\n",
        "    df_tokens_clean_texto, tokens_count_texto = analizar_texto(texto_filtrado)\n",
        "\n",
        "    print(\"Primera 10 palabras del texto:\\n\", df_tokens_clean_texto.head(10))\n",
        "    print(\"Frecuencia de palabras:\\n\", tokens_count_texto.head(30))\n",
        "\n",
        "    # Guardar resultados en CSV\n",
        "    tokens_count_texto.to_csv('/content/tokens_count_texto.csv', encoding='utf-8', index=False)\n",
        "\n",
        "    # Generar nube de palabras\n",
        "    generar_wordcloud(tokens_count_texto, \"Nube de Palabras - Palabras\")\n",
        "\n",
        "    # Análisis de bigramas NOUN-ADJ\n",
        "    data_tag_texto = bigramas_noun_adj(df_tokens_clean_texto, nlp)\n",
        "    ngram_noun_adj_count_texto = data_tag_texto['token_bi'].value_counts().rename_axis('ngram').reset_index(name='counts')\n",
        "\n",
        "    print(\"Frecuencia de bigramas NOUN-ADJ:\\n\", ngram_noun_adj_count_texto.head(30))\n",
        "\n",
        "    # Guardar bigramas en CSV\n",
        "    ngram_noun_adj_count_texto.to_csv('/content/ngram_noun_adj_count_texto.csv', encoding='utf-8', index=False)\n",
        "\n",
        "    # Generar nube de palabras para bigramas\n",
        "    generar_wordcloud(ngram_noun_adj_count_texto, \"Nube de Palabras - Bigramas\")\n",
        "\n",
        "# Suponiendo que 'texto' ya está disponible, ejecutamos la función principal\n",
        "main(texto)\n"
      ],
      "metadata": {
        "id": "1HJb7lB1EcmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_filtrado = limpiar_texto(texto)\n",
        "texto_filtrado"
      ],
      "metadata": {
        "id": "VcUjTQpkEf-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "\n",
        "def analizar_esg(texto_filtrado):\n",
        "    # Análisis de frecuencia de \"esg\"\n",
        "    doc = nlp(texto_filtrado)\n",
        "    palabras = [token.text for token in doc]\n",
        "    frecuencia_palabras = Counter(palabras)\n",
        "    frecuencia_esg = {palabra: frecuencia for palabra, frecuencia in frecuencia_palabras.items() if \"esg\" in palabra}\n",
        "\n",
        "    print(\"Frecuencia de 'esg' y derivados:\\n\", frecuencia_esg)\n",
        "\n",
        "    # Análisis de co-ocurrencias con \"esg\"\n",
        "    oraciones = list(doc.sents)\n",
        "    co_ocurrencias = []\n",
        "\n",
        "    for oracion in oraciones:\n",
        "        if any(\"esg\" in token.lemma_ for token in oracion):\n",
        "            palabras_coocurrencias = [token.text for token in oracion if token.text != \"esg\"]\n",
        "            co_ocurrencias.extend(palabras_coocurrencias)\n",
        "\n",
        "    co_ocurrencias_frecuencia = Counter(co_ocurrencias)\n",
        "    print(\"\\nPalabras más comunes que co-ocurren con 'esg':\\n\", co_ocurrencias_frecuencia.most_common(10))\n",
        "\n",
        "    # Análisis de sentimientos en contextos de \"esg\"\n",
        "    sentimientos = []\n",
        "    for oracion in oraciones:\n",
        "        if any(\"esg\" in token.lemma_ for token in oracion):\n",
        "            blob = TextBlob(oracion.text)\n",
        "            sentimientos.append(blob.sentiment.polarity)\n",
        "\n",
        "    promedio_sentimiento = np.mean(sentimientos) if sentimientos else 0\n",
        "    print(\"\\nPromedio de Sentimiento en Contextos de 'esg':\", promedio_sentimiento)\n",
        "\n",
        "    # Extracción de bigramas/trigramas que incluyan \"esg\"\n",
        "    bigramas = list(ngrams(palabras, 2))\n",
        "    trigramas = list(ngrams(palabras, 3))\n",
        "\n",
        "    bigramas_esg = [\" \".join(bigrama) for bigrama in bigramas if \"esg\" in bigrama]\n",
        "    trigramas_esg = [\" \".join(trigrama) for trigrama in trigramas if \"esg\" in trigrama]\n",
        "\n",
        "    print(\"\\nBigramas con 'esg':\\n\", bigramas_esg)\n",
        "    print(\"\\nTrigramas con 'esg':\\n\", trigramas_esg)\n",
        "\n",
        "    # Generación de resumen de oraciones clave con \"esg\"\n",
        "    oraciones_clave = [oracion.text for oracion in oraciones if any(\"esg\" in token.lemma_ for token in oracion)]\n",
        "    resumen = \" \".join(oraciones_clave[:5])  # Resumimos con las primeras 5 oraciones clave\n",
        "\n",
        "    print(\"\\nResumen de Oraciones Clave sobre 'esg':\\n\", resumen)\n",
        "\n",
        "    # Devolver los valores necesarios para la siguiente parte del análisis\n",
        "    return resumen, co_ocurrencias_frecuencia, promedio_sentimiento\n",
        "\n"
      ],
      "metadata": {
        "id": "jPmcBcBBEn77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "\n",
        "def analizar_termino(texto_filtrado, termino):\n",
        "    # Verificar si termino es una lista o una cadena y normalizar a lista\n",
        "    if isinstance(termino, str):\n",
        "        terminos = [termino.lower()]\n",
        "    else:\n",
        "        terminos = [t.lower() for t in termino]\n",
        "\n",
        "    # Análisis de frecuencia del término\n",
        "    doc = nlp(texto_filtrado)\n",
        "    palabras = [token.text.lower() for token in doc]\n",
        "    total_palabras = len(palabras)\n",
        "    frecuencia_palabras = Counter(palabras)\n",
        "    frecuencia_termino = {palabra: frecuencia for palabra, frecuencia in frecuencia_palabras.items() if any(t in palabra for t in terminos)}\n",
        "\n",
        "    print(f\"Frecuencia de {terminos} y derivados:\\n\", frecuencia_termino)\n",
        "\n",
        "    # Frecuencia relativa del término\n",
        "    frecuencia_relativa = {t: frecuencia_termino.get(t, 0) / total_palabras for t in terminos}\n",
        "    print(f\"Frecuencia relativa de {terminos}:\\n\", frecuencia_relativa)\n",
        "\n",
        "    # Análisis de co-ocurrencias con el término\n",
        "    oraciones = list(doc.sents)\n",
        "    co_ocurrencias = []\n",
        "    longitudes_oraciones = []\n",
        "\n",
        "    for oracion in oraciones:\n",
        "        if any(t in token.lemma_.lower() for t in terminos for token in oracion):\n",
        "            palabras_coocurrencias = [token.text for token in oracion if not any(t in token.text.lower() for t in terminos)]\n",
        "            co_ocurrencias.extend(palabras_coocurrencias)\n",
        "            longitudes_oraciones.append(len(oracion))\n",
        "\n",
        "    co_ocurrencias_frecuencia = Counter(co_ocurrencias)\n",
        "    print(f\"\\nPalabras más comunes que co-ocurren con {terminos}:\\n\", co_ocurrencias_frecuencia.most_common(10))\n",
        "\n",
        "    # Diversidad de contexto\n",
        "    diversidad_contexto = len(co_ocurrencias_frecuencia)\n",
        "    print(f\"Diversidad de contexto de {terminos} (número de palabras diferentes que co-ocurren): {diversidad_contexto}\")\n",
        "\n",
        "    # Análisis de sentimientos en contextos del término\n",
        "    sentimientos = []\n",
        "    for oracion in oraciones:\n",
        "        if any(t in token.lemma_.lower() for t in terminos for token in oracion):\n",
        "            blob = TextBlob(oracion.text)\n",
        "            sentimientos.append(blob.sentiment.polarity)\n",
        "\n",
        "    promedio_sentimiento = np.mean(sentimientos) if sentimientos else 0\n",
        "    varianza_sentimiento = np.var(sentimientos) if sentimientos else 0\n",
        "    print(f\"\\nPromedio de Sentimiento en Contextos de {terminos}: {promedio_sentimiento}\")\n",
        "    print(f\"Varianza de Sentimiento en Contextos de {terminos}: {varianza_sentimiento}\")\n",
        "\n",
        "    # Longitud media de las oraciones que contienen el término\n",
        "    longitud_media_oracion = np.mean(longitudes_oraciones) if longitudes_oraciones else 0\n",
        "    print(f\"Longitud media de las oraciones que contienen {terminos}: {longitud_media_oracion} palabras\")\n",
        "\n",
        "    # Extracción de bigramas/trigramas que incluyan el término\n",
        "    bigramas = list(ngrams(palabras, 2))\n",
        "    trigramas = list(ngrams(palabras, 3))\n",
        "\n",
        "    bigramas_termino = [\" \".join(bigrama) for bigrama in bigramas if any(t in bigrama for t in terminos)]\n",
        "    trigramas_termino = [\" \".join(trigrama) for trigrama in trigramas if any(t in trigrama for t in terminos)]\n",
        "\n",
        "    print(f\"\\nBigramas con {terminos}:\\n\", bigramas_termino[:10])\n",
        "    print(f\"\\nTrigramas con {terminos}:\\n\", trigramas_termino[:10])\n",
        "\n",
        "    # Generación de resumen de oraciones clave con el término\n",
        "    oraciones_clave = [oracion.text for oracion in oraciones if any(t in token.lemma_.lower() for t in terminos for token in oracion)]\n",
        "    resumen = \" \".join(oraciones_clave[:5])  # Resumimos con las primeras 5 oraciones clave\n",
        "\n",
        "    print(f\"\\nResumen de Oraciones Clave sobre {terminos}:\\n\", resumen)\n",
        "\n",
        "    # Devolver los valores necesarios para la siguiente parte del análisis\n",
        "    return {\n",
        "        \"resumen\": resumen,\n",
        "        \"frecuencia_termino\": frecuencia_termino,\n",
        "        \"frecuencia_relativa\": frecuencia_relativa,\n",
        "        \"co_ocurrencias_frecuencia\": co_ocurrencias_frecuencia,\n",
        "        \"diversidad_contexto\": diversidad_contexto,\n",
        "        \"promedio_sentimiento\": promedio_sentimiento,\n",
        "        \"varianza_sentimiento\": varianza_sentimiento,\n",
        "        \"longitud_media_oracion\": longitud_media_oracion,\n",
        "        \"bigramas_termino\": bigramas_termino,\n",
        "        \"trigramas_termino\": trigramas_termino,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "COiQX8RzEqjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "\n",
        "def limitar_palabras(texto, max_palabras=100):\n",
        "    palabras = texto.split()\n",
        "    if len(palabras) > max_palabras:\n",
        "        return \" \".join(palabras[:max_palabras]) + \"...\"\n",
        "    else:\n",
        "        return texto\n",
        "\n",
        "def analizar_termino(texto_filtrado, termino):\n",
        "    # Verificar si termino es una lista o una cadena y normalizar a lista\n",
        "    if isinstance(termino, str):\n",
        "        terminos = [termino.lower()]\n",
        "    else:\n",
        "        terminos = [t.lower() for t in termino]\n",
        "\n",
        "    # Unir los términos compuestos en una sola cadena si hay más de una palabra\n",
        "    terminos = [\" \".join(t.split()) for t in terminos]\n",
        "\n",
        "    # Análisis de frecuencia del término\n",
        "    doc = nlp(texto_filtrado)\n",
        "    palabras = [token.text.lower() for token in doc]\n",
        "    total_palabras = len(palabras)\n",
        "\n",
        "    # Frecuencia de términos compuestos\n",
        "    texto_completo = \" \".join(palabras)\n",
        "    frecuencia_termino = {t: texto_completo.count(t) for t in terminos}\n",
        "\n",
        "    print(f\"Frecuencia de {terminos} y derivados:\\n\", frecuencia_termino)\n",
        "\n",
        "    # Frecuencia relativa del término\n",
        "    frecuencia_relativa = {t: frecuencia_termino.get(t, 0) / total_palabras for t in terminos}\n",
        "    print(f\"Frecuencia relativa de {terminos}:\\n\", frecuencia_relativa)\n",
        "\n",
        "    # Análisis de co-ocurrencias con el término\n",
        "    oraciones = list(doc.sents)\n",
        "    co_ocurrencias = []\n",
        "    longitudes_oraciones = []\n",
        "\n",
        "    for oracion in oraciones:\n",
        "        if any(t in oracion.text.lower() for t in terminos):\n",
        "            palabras_coocurrencias = [token.text for token in oracion if not any(t in token.text.lower() for t in terminos)]\n",
        "            co_ocurrencias.extend(palabras_coocurrencias)\n",
        "            longitudes_oraciones.append(len(oracion))\n",
        "\n",
        "    co_ocurrencias_frecuencia = Counter(co_ocurrencias)\n",
        "    print(f\"\\nPalabras más comunes que co-ocurren con {terminos}:\\n\", co_ocurrencias_frecuencia.most_common(10))\n",
        "\n",
        "    # Diversidad de contexto\n",
        "    diversidad_contexto = len(co_ocurrencias_frecuencia)\n",
        "    print(f\"Diversidad de contexto de {terminos} (número de palabras diferentes que co-ocurren): {diversidad_contexto}\")\n",
        "\n",
        "    # Análisis de sentimientos en contextos del término\n",
        "    sentimientos = []\n",
        "    for oracion in oraciones:\n",
        "        if any(t in oracion.text.lower() for t in terminos):\n",
        "            blob = TextBlob(oracion.text)\n",
        "            sentimientos.append(blob.sentiment.polarity)\n",
        "\n",
        "    promedio_sentimiento = np.mean(sentimientos) if sentimientos else 0\n",
        "    varianza_sentimiento = np.var(sentimientos) if sentimientos else 0\n",
        "    print(f\"\\nPromedio de Sentimiento en Contextos de {terminos}: {promedio_sentimiento}\")\n",
        "    print(f\"Varianza de Sentimiento en Contextos de {terminos}: {varianza_sentimiento}\")\n",
        "\n",
        "    # Longitud media de las oraciones que contienen el término\n",
        "    longitud_media_oracion = np.mean(longitudes_oraciones) if longitudes_oraciones else 0\n",
        "    print(f\"Longitud media de las oraciones que contienen {terminos}: {longitud_media_oracion} palabras\")\n",
        "\n",
        "    # Extracción de bigramas/trigramas que incluyan el término\n",
        "    bigramas = list(ngrams(palabras, 2))\n",
        "    trigramas = list(ngrams(palabras, 3))\n",
        "\n",
        "    bigramas_termino = [\" \".join(bigrama) for bigrama in bigramas if any(t in \" \".join(bigrama) for t in terminos)]\n",
        "    trigramas_termino = [\" \".join(trigrama) for trigrama in trigramas if any(t in \" \".join(trigrama) for t in terminos)]\n",
        "\n",
        "    print(f\"\\nBigramas con {terminos}:\\n\", bigramas_termino[:10])\n",
        "    print(f\"\\nTrigramas con {terminos}:\\n\", trigramas_termino[:10])\n",
        "\n",
        "    # Generación de resumen de oraciones clave con el término\n",
        "    oraciones_clave = [oracion.text for oracion in oraciones if any(t in oracion.text.lower() for t in terminos)]\n",
        "    resumen = \" \".join(oraciones_clave[:5])  # Resumimos con las primeras 5 oraciones clave\n",
        "    resumen_limitado = limitar_palabras(resumen, 300)\n",
        "\n",
        "    print(f\"\\nResumen de Oraciones Clave sobre {terminos} (Limitado a 100 palabras):\\n\", resumen_limitado)\n",
        "\n",
        "    # Devolver los valores necesarios para la siguiente parte del análisis\n",
        "    return {\n",
        "        \"resumen\": resumen_limitado,\n",
        "        \"frecuencia_termino\": frecuencia_termino,\n",
        "        \"frecuencia_relativa\": frecuencia_relativa,\n",
        "        \"co_ocurrencias_frecuencia\": co_ocurrencias_frecuencia,\n",
        "        \"diversidad_contexto\": diversidad_contexto,\n",
        "        \"promedio_sentimiento\": promedio_sentimiento,\n",
        "        \"varianza_sentimiento\": varianza_sentimiento,\n",
        "        \"longitud_media_oracion\": longitud_media_oracion,\n",
        "        \"bigramas_termino\": bigramas_termino,\n",
        "        \"trigramas_termino\": trigramas_termino,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "RgDhPTRAEsyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que 'texto_filtrado' ya está disponible\n",
        "resultados = analizar_termino(texto_filtrado, \"inteligencia artificial\")\n",
        "\n",
        "# Ahora 'resultados' contiene todas las métricas interesantes# Paso 2: Preparar los datos para el LLM\n",
        "resumen_oraciones_clave = resultados[\"resumen\"]\n",
        "co_ocurrencias = \", \".join([f\"{palabra}: {frecuencia}\" for palabra, frecuencia in resultados[\"co_ocurrencias_frecuencia\"].most_common(10)])\n",
        "sentimientos = f\"{resultados['promedio_sentimiento']:.10f}\""
      ],
      "metadata": {
        "id": "kgbuDFrCEv3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#No correr\n",
        "import openai\n",
        "\n",
        "# Configura tu clave API de OpenAI\n",
        "#openai.api_key = \"\"\n",
        "\n",
        "# Función para generar un resumen utilizando un LLM con la nueva API\n",
        "def generar_resumen_llm(termino, oraciones_clave, co_ocurrencias, sentimientos):\n",
        "    prompt = f\"\"\"\n",
        "    Estoy analizando un texto para entender cómo se discute el término '{termino}'.\n",
        "    A continuación te doy un resumen de las oraciones clave donde aparece la palabra \"{termino}\" o sus derivados:\n",
        "\n",
        "    {oraciones_clave}\n",
        "\n",
        "    Además, he identificado las siguientes palabras que suelen aparecer junto a \"{termino}\":\n",
        "\n",
        "    {co_ocurrencias}\n",
        "\n",
        "    El sentimiento general en estos contextos es {sentimientos}.\n",
        "\n",
        "    Con base en esta información, ¿podrías generar un resumen detallado que\n",
        "    describa cómo se trata el término '{termino}' en este texto?\n",
        "    Por favor, incluye cualquier observación importante sobre el tono y las ideas\n",
        "    principales relacionadas con '{termino}'.\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "      model=\"gpt-4\",  # Usa el modelo que prefieras\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Eres un asistente que ayuda a analizar textos complejos.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "      ],\n",
        "      max_tokens=500,  # Ajusta según lo que necesites\n",
        "      temperature=0.7,\n",
        "      top_p=1,\n",
        "      n=1,\n",
        "    )\n",
        "\n",
        "    resumen_generado = response['choices'][0]['message']['content'].strip()\n",
        "    return resumen_generado\n",
        "\n"
      ],
      "metadata": {
        "id": "BCRnp0uKEyeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genera el resumen utilizando el LLM\n",
        "resumen_inteligencia_artificial = generar_resumen_llm(\"inteligencia artificial\", resumen_oraciones_clave, co_ocurrencias, sentimientos)\n",
        "# Imprimir el resumen generado\n",
        "print(resumen_inteligencia_artificial)"
      ],
      "metadata": {
        "id": "2Ai3KMvVE27c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}